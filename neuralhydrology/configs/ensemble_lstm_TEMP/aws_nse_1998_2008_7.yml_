# --- Experiment configurations --------------------------------------------------------------------
# https://stefanbauer.me/articles/how-to-keep-your-git-fork-up-to-date#:~:text=Merge%20the%20upstream%20with%20your,can%20just%20merge%20the%20changes.&text=With%20that%2C%20you%20merge%20the,and%20merging%20in%20one%20step.

# experiment name, used as folder name
experiment_name: lstm_ensemble7_nse_LANE

# place to store run directory (if empty runs are stored in code_dir/runs/)
run_dir: /cats/datastore/data/runs/ensemble_lstm_TEMP

# files to specify training, validation and test basins (relative to code root or absolute path)
train_basin_file: /home/tommy/neuralhydrology/data/camels_gb_basin_list.txt
validation_basin_file: /home/tommy/neuralhydrology/data/camels_gb_basin_list.txt
test_basin_file: /home/tommy/neuralhydrology/data/camels_gb_basin_list.txt

# training, validation and test time periods (format = 'dd/mm/yyyy')
train_start_date: '01/01/1988'
train_end_date: '31/12/1997'
validation_start_date: '01/10/1975'
validation_end_date: '30/09/1980'
test_start_date: '01/01/1998'
test_end_date: '31/12/2008'

# which GPU (id) to use [in format of cuda:0, cuda:1 etc, or cpu or None]
device: cuda:0

# fixed seed, leave empty to use a random seed
seed: 7

# --- Validation configuration ---------------------------------------------------------------------

# specify after how many epochs to perform validation
validate_every: 5

# specify how many random basins to use for validation
validate_n_random_basins: 10

# specify which metrics to calculate during validation (see neuralhydrology.evaluation.metrics)
# this can either be a list or a dictionary. If a dictionary is used, the inner keys must match the name of the
# target_variable specified below. Using dicts allows for different metrics per target variable.
metrics:
- NSE
- KGE

# --- Model configuration --------------------------------------------------------------------------

# base model type [lstm, ealstm, cudalstm, embcudalstm, mtslstm]
# (has to match the if statement in modelzoo/__init__.py)
model: cudalstm

# prediction head [regression]. Define the head specific parameters below
head: regression

# ----> Regression settings <----
output_activation: linear

# ----> General settings <----

# Number of cell states of the LSTM
hidden_size: 64

# Initial bias value of the forget gate
initial_forget_bias: 3

# Dropout applied to the output of the LSTM
output_dropout: 0.4

# --- Training configuration -----------------------------------------------------------------------

# specify optimizer [Adam]
optimizer: Adam

# specify loss [MSE, NSE, RMSE]
loss: NSE

# specify learning rates to use starting at specific epochs (0 is the initial learning rate)
learning_rate:
    0: 1e-3
    10: 5e-4
    20: 1e-4

# Mini-batch size
batch_size: 256

# Number of training epochs
epochs: 30

# If a value, clips the gradients during training to that norm.
clip_gradient_norm: 1

# Defines which time steps are used to calculate the loss. Can't be larger than seq_length.
# If use_frequencies is used, this needs to be a dict mapping each frequency to a predict_last_n-value, else an int.
predict_last_n: 1

# Length of the input sequence
# If use_frequencies is used, this needs to be a dict mapping each frequency to a seq_length, else an int.
seq_length: 365

# Number of parallel workers used in the data pipeline
num_workers: 8

# Log the training loss every n steps
log_interval: 10

# If true, writes logging results into tensorboard file
log_tensorboard: True

# If a value and greater than 0, logs n random basins as figures during validation
log_n_figures: 1

# Save model weights every n epochs
save_weights_every: 1


# --- Data configurations --------------------------------------------------------------------------

# which data set to use [camels_us, camels_gb, global, hourly_camels_us]
dataset: camels_gb

# Path to data set root
data_dir: /cats/datastore/data/CAMELS_GB_DATASET

# Path to pre-processed hdf5 file here (and corresponding scaler pickle file). Leave empty to create
# new hdf5 file
h5_file:
scaler_file:

# whether to load the entire data into memory or not
cache_validation_data: True

# Forcing product [daymet, maurer, maurer_extended, nldas, nldas_extended, nldas_hourly]
# can be either a list of forcings or a single forcing product
# forcings: maurer_extended

# which columns to use as target
target_variables:
- discharge_spec

# variables to use as time series input (names match the data file column headers)
dynamic_inputs:
- precipitation
- pet
- temperature

# Which CAMELS attributes to use. Leave empty if none should be used
# static_inputs:
camels_attributes:
- area
- elev_mean
- dpsbar
- sand_perc
- silt_perc
- clay_perc
- porosity_hypres
- conductivity_hypres
- soil_depth_pelletier
- dwood_perc
- ewood_perc
- crop_perc
- urban_perc
- reservoir_cap
- p_mean
- pet_mean
- p_seasonality
- frac_snow
- high_prec_freq
- low_prec_freq
- high_prec_dur
- low_prec_dur


# clip negative predictions to zero for all variables listed below. Should be a list, even for single variables.
clip_target_to_zero:
- discharge_spec

# zero_center_target: True -- DEPRECEATED
custom_normalization:

use_basin_id_encoding: False